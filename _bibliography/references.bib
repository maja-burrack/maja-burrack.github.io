@book{BDA3,
  author    = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  title     = {Bayesian Data Analysis},
  edition   = {3},
  year      = {2013},
  publisher = {Chapman and Hall/CRC},
  address   = {Boca Raton},
  isbn      = {978-1-4398-4095-5}
}

@book{AllOfStatistics,
  author    = {Wasserman, Larry},
  title     = {All of Statistics: A Concise Course in Statistical Inference},
  year      = {2004},
  publisher = {Springer},
  address   = {New York},
  isbn      = {978-0-387-40272-7}
}

@online{hoffmanGelmanNUTS2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  date = {2011-11-18},
  eprint = {1111.4246},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1111.4246},
  url = {http://arxiv.org/abs/1111.4246},
  urldate = {2025-05-10},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
}

@book{ElementsStatisticalLearning,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  lccn = {Q325.5 .H39 2009},
  keywords = {Bioinformatics,Computational intelligence,Data mining,Forecasting,Inference,Machine learning,Methodology,Statistics}
}

@book{gelmanMultilevel,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2021},
  series = {Analytical Methods for Social Research},
  edition = {23rd printing},
  publisher = {Cambridge Univ. Press},
  address = {Cambridge},
  isbn = {978-0-521-68689-1 978-0-521-86706-1},
  langid = {english}
}

