---
layout: post
title: SHAP shit
subtitle:
---

Imagine you've built a powerful, complex model that predicts outcome $y$. Although it performs very well, when you present it to the relevant stakeholders, they ask you *why* the model predicts $y_1$ instead of $y_2$. What "drives" the outcome (as the usual lingo goes)?

But you're stomped; the complex model you've built isn't as interpretable as, say, linear regression. You can't easily determine how changing one input variable affects the prediction or quantify how each feature contributes to the final output. For example, you can't confidently say weather a input variable $x_1$ has a positive effect on the outcome $\hat{y}$. 

It turns out there exists a framework that tries to answer exactly this: How much does each feature contribute towards the predictions? 

The framework is called SHAP, which stands for SHapley Additive Explanations. It's a general framework for "explaining" machine learning models (often dubbed "explainable AI" or xAI), which is model-agnostic--meaning it works for all types of models, from tree-based models like XGBoost and CatBoost to neural networks.

As the name suggests, SHAP is based on an old game-theoretic concept called [Shapley values](https://en.wikipedia.org/wiki/Shapley_value).

Shapley values, and by extension SHAP values, are a way to *fairly* distribute the total gains (or "payout") of a collaborative game to the participants of the game. 

We can relate this to machine learning models by thinking of a model as a game played by the input variables. The features work together to obtain a model output $\hat{y}$. 

