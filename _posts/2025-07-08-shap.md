---
layout: post
title: SHAP shit
subtitle:
---

Let's say you've built a really cool and elaborate model that predicts outcome $y$. It's very accurate in its predictions, but when you present it to the relevant stakeholders, they ask you *why* the model predicts $y_1$ instead of $y_2$. What "drives" the outcome (as the usual lingo goes)?

You're stomped because the model you built is a complex model, so it's not easily interpretable like, e.g., a linear model is. You can't readily deduce what happens to the predicted outcome if you tweak a specific input variable one way or another, and you can't infer how each input variable "contributes" to the final prediction.

It turns out there exists a framework that tries to answer exactly this: How much does each feature contribute towards the predictions? 

The framework is called SHAP, which stands for SHapley Additive Explanations. It's a general framework for "explaining" machine learning models (often dubbed "explainable AI" or xAI), which is model-agnostic, meaning it works for all types of models including tree-based models like xgboost and catboost, and neural networks. (Insert intro to structure of post here?)

As the name suggests, SHAP is based on an old game-theoretic concept called [Shapley values](https://en.wikipedia.org/wiki/Shapley_value).

Shapley values, and by extension SHAP values, are a way to *fairly* distribute the payout, or total gains, of a collaborative game to the participants of the game. 

We can relate this to machine learning models by thinking of a model as a game played by the input variables. The features work together to obtain a model output $f(X) = \hat{y}$. 

